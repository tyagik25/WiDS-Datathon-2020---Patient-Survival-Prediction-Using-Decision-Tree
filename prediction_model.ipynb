{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport sys\n\nnp.set_printoptions(threshold=sys.maxsize)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Importing the relevant Libraries**\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Loading dataset\ntrain_df = pd.read_csv(\"../input/widsdatathon2020/training_v2.csv\")\ntest_df = pd.read_csv(\"../input/widsdatathon2020/unlabeled.csv\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(train_df.nunique)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isna()\ntrain_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.isna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['hospital_death'].dtype","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['hospital_death'].dtype","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's print the sum of missing and uniques values of all the columns**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_columns_properties(df):\n    for i, col in enumerate(df.columns.tolist()):\n         print('\\n ({} {})  Missing: {}  UniqValsSz: {}'.format(i,col, df[col].isnull().sum() ,df[col].unique().size))\n    print('\\n')\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_columns_properties(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_columns_properties(test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Selecting Categorical Columns**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_train_df = train_df.select_dtypes(include='object')\ncat_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_test_df = test_df.select_dtypes(include='object')\ncat_test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_test_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Printing Unique Values per column* **","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_columns_uniqvals(df):\n    for i, col in enumerate(df.columns.tolist()):\n         print('\\n ({} {}) Uniq: {}'.format(i,col, df[col].unique() ))\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_columns_uniqvals(cat_test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Splitting the data**\n\nWe will split the data into two parts: 80% of Training set and 20% of Validation set. We will use Validation set for prediction and deciding which model/approach works better using the validation score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# copy the data\ntrain = train_df.copy()\n\n# Select target\ny = train['hospital_death']\n\n# To keep things simple, we'll use only numerical predictors\npredictors = train.drop(['hospital_death'], axis=1)\nX = predictors.select_dtypes(exclude=['object'])\n\n# Divide data into training and validation subsets\nX_train, X_valid, y_train, y_valid = train_test_split(X,y, train_size=0.8, test_size=0.2,\n                                                      random_state=0)\n\nX_train.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_valid.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Handling missing values using Imputation**\n\nFor categorical, we can fill with the most frequent value for that column.\nFor numerical, we can fill with mean or median value for that column. New values filled may be far away from what actual values should be.\nIt is of two types. Simple Imputation - fills some value and does not remember which all positions had been missing.\n\nImputation with extension - fills some value and remembers which all values are missing. New columns are created to store which positions had missing values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\n# Imputation\nmy_imputer = SimpleImputer()\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train)) #fit_transform is used for calculating the mean from columns and then replacing the missing values\nimputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n\n# Imputation removed column names; put them back\nimputed_X_train.columns = X_train.columns\nimputed_X_valid.columns = X_valid.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_columns_properties(imputed_X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_columns_properties(imputed_X_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since there are no misisng values left, now we can apply Machine Learning Algorithm","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n\n# Define model; Specify a number for random_state to ensure the same results in each run.\ndt_model = DecisionTreeRegressor(random_state=1)\n\n# Fit model using Training data\ndt_model.fit(imputed_X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get predicted values on validation data\npredicted_values = dt_model.predict(imputed_X_valid)\n\n# Find difference\nscore = mean_absolute_error(y_valid, predicted_values)\nprint('MAE:', score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find difference\nscore = mean_absolute_error(y_valid, predicted_values)\nprint('MAE:', score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test_df.copy()\n\n#Separate target\ny_test = test['hospital_death']\n\n# To keep things simple, we will only use numerical predictors\npredictors_test = test.drop(['hospital_death'], axis=1)\nX_test = predictors_test.select_dtypes(exclude=['object'])\n\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imputation\nmy_imputer = SimpleImputer()\nimputed_X_test = pd.DataFrame(my_imputer.fit_transform(X_test))\n\n# Imputation removed column names; put them back\nimputed_X_test.columns = X_test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputed_X_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get predictions on test data\npreds = dt_model.predict(imputed_X_test)\n\n# Save predictions in format used for competition scoring\noutput = pd.DataFrame({'encounter_id': imputed_X_test.encounter_id,\n                       'hospital_death': preds},dtype=np.int32)\nprint(output)\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}